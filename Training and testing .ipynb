{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5e7f71",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m gpus \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset_logical_device_configuration(\n\u001b[1;32m---> 34\u001b[0m         \u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     35\u001b[0m         [tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mLogicalDeviceConfiguration(memory_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5632\u001b[39m)])\n\u001b[0;32m     36\u001b[0m     logical_gpus \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_logical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gpus), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhysical GPUs,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(logical_gpus), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogical GPUs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Feb 17 13:11:12 2023\n",
    "\n",
    "@author: Debra Hogue\n",
    "\n",
    "Modified RankNet by Lv et al. to use Tensorflow not Pytorch\n",
    "and added additional comments to explain methods\n",
    "\n",
    "Paper: Simultaneously Localize, Segment and Rank the Camouflaged Objects by Lv et al.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, layers, losses\n",
    "import numpy as np\n",
    "import os, argparse\n",
    "from datetime import datetime\n",
    "from Attention.ResNet_models import Generator\n",
    "from data import get_loader\n",
    "from utils import adjust_lr, AvgMeter\n",
    "from scipy import misc\n",
    "import cv2\n",
    "from data import test_dataset\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "import tensorflow.keras.applications.resnet50 as models # instantiates the ResNet50 architecture \n",
    "\n",
    "from utils import l2_regularisation\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=5632)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "tf.config.optimizer.set_experimental_options({\"Memory optimizer\": True, \"Layout optimizer\": True})\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203e9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--epoch', type=int, default=1, help='epoch number')\n",
    "# parser.add_argument('--lr_gen', type=float, default=2.5e-5, help='learning rate for generator')\n",
    "# parser.add_argument('--batchsize', type=int, default=2, help='training batch size')\n",
    "# parser.add_argument('--trainsize', type=int, default=480, help='training dataset size')\n",
    "# parser.add_argument('--decay_rate', type=float, default=0.9, help='decay rate of learning rate')\n",
    "# parser.add_argument('--decay_epoch', type=int, default=40, help='every n epochs decay learning rate')\n",
    "# parser.add_argument('--feat_channel', type=int, default=32, help='reduced channel of saliency feat')\n",
    "# opt = parser.parse_args()\n",
    "# print('Generator Learning Rate: {}'.format(opt.lr_gen))\n",
    "epoch = 80\n",
    "decay_rate = 0.97\n",
    "decay_epoch = 65\n",
    "batchsize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744bfe3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# build models\n",
    "generator = Generator(channel=32)\n",
    "\n",
    "\n",
    "# generator_params = generator.parameters()\n",
    "# generator_optimizer = tf.optimizers.Adam(generator_params, opt.lr_gen)\n",
    "\n",
    "\n",
    "image_root = './dataset/train/Imgs/'\n",
    "gt_root = './dataset/train/GT/'\n",
    "fix_root = './dataset/train/Fix/'\n",
    "\n",
    "# train_loader = get_loader(image_root, gt_root, fix_root,batchsize=opt.batchsize, trainsize=opt.trainsize)\n",
    "# total_step = len(train_loader)\n",
    "\n",
    "CE = losses.BinaryCrossentropy(from_logits=True)\n",
    "mse_loss = losses.MeanSquaredError()\n",
    "size_rates = [0.75,1,1.25]  # multi-scale training\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    padded = tf.pad(mask,tf.constant([[0,0],[15,15],[15,15],[0,0]]))\n",
    "    pooled =tf.nn.avg_pool2d(padded, ksize=31, strides=1, padding=\"VALID\")\n",
    "    weit  = 1+5*tf.abs(pooled-mask)\n",
    "    weit = tf.squeeze(weit,[3])\n",
    "    wbce= tf.nn.sigmoid_cross_entropy_with_logits(mask,pred)\n",
    "   \n",
    "    wbce= tf.math.reduce_mean(wbce)\n",
    "    wbce  = tf.math.reduce_sum((weit*wbce),axis=[1,2]) /tf.reduce_sum(weit,axis=[1,2])\n",
    "    mask =tf.squeeze(mask,[3])\n",
    "    pred  = tf.math.sigmoid(pred)\n",
    "    pred = tf.squeeze(pred,[3])\n",
    "    inter = tf.math.reduce_sum((pred*mask)*weit,axis=[1,2])\n",
    "    union = tf.math.reduce_sum((pred+mask)*weit, axis=[1,2])\n",
    "    wiou  = 1-(inter+1)/(union-inter+1)\n",
    "    return tf.math.reduce_mean(wbce+wiou)\n",
    "\n",
    "\n",
    "        \n",
    "             \n",
    "def loss_function(y_true,y_pred):\n",
    "    gts, fixs = tf.unstack(y_true,2,0)\n",
    "    gts, _ = tf.split(gts, [1,2], 3)\n",
    "    fixs, _ = tf.split(fixs, [1,2], 3)\n",
    "    fix_pred, cod_pred1, cod_pred2 = tf.unstack(y_pred,num=3,axis=0)\n",
    "    fix_loss = mse_loss(tf.keras.activations.sigmoid(fix_pred),fixs)\n",
    "    cod_loss1 = structure_loss(cod_pred1, gts)\n",
    "    cod_loss2 = structure_loss(cod_pred2, gts)\n",
    "    test= fix_loss + cod_loss1 + cod_loss2\n",
    "    return  fix_loss + cod_loss1 + cod_loss2\n",
    "    \n",
    "def on_epoch_end( epoch, lr):\n",
    "    decay = decay_rate ** (epoch // decay_epoch)\n",
    "    new_lr = lr * decay\n",
    "    print(\"\\nEpoch: {}. Reducing Learning Rate from {} to {}\".format(epoch, lr, new_lr))\n",
    "    return new_lr\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch='10, 15')\n",
    "    \n",
    "    op= tf.keras.optimizers.Adam(learning_rate=2.5e-5, name='Adam')\n",
    "    \n",
    "    generator.compile(optimizer=op, loss=loss_function, run_eagerly=False)\n",
    "    \n",
    "    \n",
    "    data = get_loader(image_root, gt_root, fix_root, 480, batchsize, size_rates)\n",
    "    print(len(data))\n",
    "    generator.fit(x=data,batch_size=batchsize, epochs=epoch, verbose='auto' , callbacks=[tensorboard_callback, tf.keras.callbacks.LearningRateScheduler(on_epoch_end)])\n",
    "     \n",
    "     \n",
    "    save_path = 'models/Resnet/'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dataset_path = './dataset/test/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "print(\"Generator is build\")\n",
    "generator.save(\"results/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_datasets = ['Mine', 'CAMO']\n",
    "for dataset in test_datasets:\n",
    "    save_path = './results/ResNet50/' + dataset + '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    image_root = dataset_path + dataset + '/Imgs/'\n",
    "    test_loader = test_dataset(image_root, 480)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        print(i)\n",
    "        image, HH, WW, name = test_loader.load_data()\n",
    "        ans = generator(image)\n",
    "        _,generator_pred, _  = tf.unstack(ans,num=3,axis=0)\n",
    "        res = generator_pred\n",
    "        res = tf.image.resize(res, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        res = tf.math.sigmoid(res).numpy().squeeze()\n",
    "        res = 255*(res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        print(save_path+name)\n",
    "        cv2.imwrite(save_path+name, res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4575b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generator2 = Generator(channel=32)\n",
    "generator2 =  tf.keras.models.load_model('./results/model', custom_objects={'loss_function': loss_function})\n",
    "for dataset in test_datasets:\n",
    "    save_path = './results/small/' + dataset + '/'\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    image_root = dataset_path + dataset + '/Imgs/'\n",
    "    test_loader = test_dataset(image_root, 480)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        print(i)\n",
    "        image, HH, WW, name = test_loader.load_data()\n",
    "        ans = generator2(image)\n",
    "        _,generator_pred, _  = tf.unstack(ans,num=3,axis=0)\n",
    "        res = generator_pred\n",
    "        res = tf.image.resize(res, size=tf.constant([WW,HH]), method=tf.image.ResizeMethod.BILINEAR)\n",
    "        res = tf.math.sigmoid(res).numpy().squeeze()\n",
    "        res = 255*(res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        print(save_path+name)\n",
    "        cv2.imwrite(save_path+name, res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
